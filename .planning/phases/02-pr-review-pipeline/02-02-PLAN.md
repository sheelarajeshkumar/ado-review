---
phase: 02-pr-review-pipeline
plan: 02
type: execute
wave: 2
depends_on: ["02-01"]
files_modified:
  - lib/review/llm-reviewer.ts
  - lib/review/comment-mapper.ts
  - lib/review/orchestrator.ts
  - entrypoints/background.ts
autonomous: true

must_haves:
  truths:
    - "A single file can be sent to OpenAI and a validated structured review is returned"
    - "LLM findings are mapped to valid ADO thread positions with line number validation"
    - "The orchestrator reviews files sequentially, posts comments, and reports progress"
    - "If one file fails after retries, the orchestrator continues to the next file"
    - "The service worker handles port connections and runs the review pipeline"
    - "Progress messages flow from service worker to content script over the port"
    - "A summary comment is generated and posted after all files are reviewed"
  artifacts:
    - path: "lib/review/llm-reviewer.ts"
      provides: "Single-file LLM review via AI SDK generateText + Output.object"
      exports: ["reviewSingleFile"]
    - path: "lib/review/comment-mapper.ts"
      provides: "Finding-to-ADO-thread mapping with line validation"
      exports: ["postFileFindings", "buildSummaryMarkdown"]
    - path: "lib/review/orchestrator.ts"
      provides: "Top-level review coordination with progress reporting"
      exports: ["runReview"]
    - path: "entrypoints/background.ts"
      provides: "Port connection handler for review sessions"
  key_links:
    - from: "lib/review/llm-reviewer.ts"
      to: "ai (Vercel AI SDK)"
      via: "generateText with Output.object and FileReviewSchema"
      pattern: "generateText.*Output\\.object"
    - from: "lib/review/llm-reviewer.ts"
      to: "lib/review/schemas.ts"
      via: "FileReviewSchema import for structured output"
      pattern: "FileReviewSchema"
    - from: "lib/review/orchestrator.ts"
      to: "lib/review/llm-reviewer.ts"
      via: "reviewSingleFile call per file"
      pattern: "reviewSingleFile"
    - from: "lib/review/orchestrator.ts"
      to: "lib/review/comment-mapper.ts"
      via: "postFileFindings and buildSummaryMarkdown calls"
      pattern: "postFileFindings|buildSummaryMarkdown"
    - from: "lib/review/orchestrator.ts"
      to: "lib/review/retry.ts"
      via: "retryWithBackoff wrapping LLM calls"
      pattern: "retryWithBackoff"
    - from: "entrypoints/background.ts"
      to: "lib/review/orchestrator.ts"
      via: "runReview call from port message handler"
      pattern: "runReview"
    - from: "entrypoints/background.ts"
      to: "browser.runtime.onConnect"
      via: "Port listener for review session"
      pattern: "onConnect"
---

<objective>
Build the review pipeline core: LLM reviewer that calls OpenAI via Vercel AI SDK, comment mapper that posts findings to ADO, orchestrator that coordinates the file-by-file loop with progress and error isolation, and background service worker port handler.

Purpose: This is the pipeline engine. After this plan, the review flow works end-to-end from a programmatic trigger -- a port message triggers a full review cycle that fetches files, reviews them, posts comments, and reports progress. Plan 02-03 connects the UI.

Output: 4 files -- `lib/review/llm-reviewer.ts`, `lib/review/comment-mapper.ts`, `lib/review/orchestrator.ts`, updated `entrypoints/background.ts`
</objective>

<execution_context>
@/Users/81169560/.claude/get-shit-done/workflows/execute-plan.md
@/Users/81169560/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/02-pr-review-pipeline/02-RESEARCH.md
@.planning/phases/02-pr-review-pipeline/02-01-SUMMARY.md

@lib/review/schemas.ts
@lib/review/types.ts
@lib/review/retry.ts
@lib/review/prompt-builder.ts
@lib/review/file-filter.ts
@lib/ado-api/client.ts
@lib/ado-api/pull-requests.ts
@lib/ado-api/file-content.ts
@lib/ado-api/threads.ts
@lib/ado-api/types.ts
@shared/types.ts
@shared/messages.ts
@shared/storage.ts
@entrypoints/background.ts
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create LLM reviewer and comment mapper</name>
  <files>
    lib/review/llm-reviewer.ts
    lib/review/comment-mapper.ts
  </files>
  <action>
**Create `lib/review/llm-reviewer.ts`:**

This module sends a single file to OpenAI and returns validated structured output. Uses Vercel AI SDK 6.x pattern: `generateText` with `Output.object()`.

Exported function:
```typescript
export async function reviewSingleFile(
  filePath: string,
  fileContent: string,
  changeType: string,
  apiKey: string,
): Promise<FileReview>
```

Implementation:
1. Import `generateText`, `Output` from `'ai'` and `createOpenAI` from `'@ai-sdk/openai'`.
2. Import `FileReviewSchema`, `FileReview` from `./schemas`.
3. Import `buildSystemPrompt`, `buildFileReviewPrompt` from `./prompt-builder`.
4. Create OpenAI provider: `const openai = createOpenAI({ apiKey })`.
5. Call `generateText` with:
   - `model: openai('gpt-4o')`
   - `output: Output.object({ schema: FileReviewSchema })`
   - `system: buildSystemPrompt()`
   - `prompt: buildFileReviewPrompt(filePath, fileContent, changeType)`
   - `maxTokens: 4000`
6. Destructure `{ output }` from the result.
7. If `!output`, throw `new Error('LLM returned no structured output for ${filePath}')`.
8. Return `output` (already typed as FileReview by AI SDK).

IMPORTANT: Do NOT import `NoObjectGeneratedError` -- just check for null output. The AI SDK validates against the Zod schema automatically.

IMPORTANT: Use `createOpenAI` (NOT `openai` from the package). The `createOpenAI` factory allows passing the API key at runtime, which is essential since the key comes from extension storage.

**Create `lib/review/comment-mapper.ts`:**

This module handles two things: (1) posting a file's findings as inline comments, filtering out invalid line numbers, and (2) building the final summary markdown.

Exported functions:

1. `postFileFindings(prInfo: PrInfo, filePath: string, findings: Finding[], iterationId: number): Promise<{ posted: number; dropped: number }>`
   - Import `postInlineComment` from `@/lib/ado-api/threads`.
   - Import `Finding` from `./schemas`, `PrInfo` from `@/shared/types`.
   - For each finding: validate that `finding.line` is a positive integer (> 0). If not, increment `dropped` counter and skip (addresses Pitfall 3 -- LLM may return invalid line numbers). If valid, call `postInlineComment(prInfo, filePath, finding, iterationId)`. Wrap each post in try/catch -- if a single comment fails to post, log the error and increment `dropped`, but continue with remaining findings.
   - Return `{ posted, dropped }` counts.

2. `buildSummaryMarkdown(results: SingleFileResult[], prTitle: string): string`
   - Import `SingleFileResult` from `./types`.
   - Build a markdown string for the PR-level summary comment.
   - Format:
     ```
     ## PEP Review Summary

     **PR:** {prTitle}

     ### Overview
     - **Files reviewed:** {count}
     - **Files skipped:** {count} (non-code files)
     - **Files with errors:** {count}
     - **Total findings:** {count} ({critical} Critical, {warning} Warning, {info} Info)

     ### File Results

     | File | Findings | Status |
     |------|----------|--------|
     | `path/to/file.ts` | 3 (1C, 2W) | Reviewed |
     | `path/to/other.ts` | 0 | Clean |
     | `path/to/broken.ts` | - | Error: {message} |

     ---
     *Generated by PEP Review*
     ```
   - Count severities by iterating over each result's findings array.
   - For the table: show abbreviated severity counts like `2 (1C, 1W)` where C=Critical, W=Warning, I=Info. Only show non-zero severity letters.
   - For error files: show "Error: {truncated error message}" in status column.
   - For skipped files: do NOT include in the table (they were filtered before review).
  </action>
  <verify>
Run `pnpm exec tsc --noEmit` -- must pass. Verify `llm-reviewer.ts` imports from `ai` and `@ai-sdk/openai`. Verify `comment-mapper.ts` imports `postInlineComment` from `@/lib/ado-api/threads`.
  </verify>
  <done>
llm-reviewer.ts sends a single file to OpenAI via AI SDK generateText with Output.object and Zod-validated FileReviewSchema. comment-mapper.ts posts findings as inline comments with line validation (drops invalid lines) and builds a markdown summary table with severity counts. Both type-check successfully.
  </done>
</task>

<task type="auto">
  <name>Task 2: Create orchestrator and background port handler</name>
  <files>
    lib/review/orchestrator.ts
    entrypoints/background.ts
  </files>
  <action>
**Create `lib/review/orchestrator.ts`:**

This is the top-level review coordinator. It takes a PrInfo + progress callback, and runs the entire review pipeline: fetch PR details, get changes, filter files, review each file, post comments, build and post summary.

Exported function:
```typescript
export async function runReview(
  prInfo: PrInfo,
  onProgress: (msg: PortMessage) => void,
): Promise<void>
```

Implementation (follow the data flow from research section "Review Pipeline Data Flow"):

1. **Get API key:** Call `getOpenAiApiKey()` from `@/shared/storage`. If null, send `REVIEW_ERROR` with message "OpenAI API key not configured. Set it in extension options." and return.

2. **Fetch PR details:** Call `getPrDetails(prInfo)`. Wrap in try/catch -- on failure, send `REVIEW_ERROR` and return.

3. **Get latest iteration:** Call `getLatestIterationId(prInfo)`.

4. **Get changed files:** Call `getChangedFiles(prInfo, iterationId)`.

5. **Filter files:** Import `shouldSkipFile` and `shouldSkipByChangeType` from `./file-filter`. Also import `CHANGE_TYPE_MAP` from `@/lib/ado-api/types`. For each change:
   - Map `change.changeType` number to string via CHANGE_TYPE_MAP.
   - If `shouldSkipByChangeType(changeTypeStr)` or `shouldSkipFile(change.item.path)`, mark as skipped.
   - Collect reviewable files and skipped files separately.

6. **Report total:** Send `REVIEW_PROGRESS` with totalFiles = reviewable count, fileIndex 0, currentFile '', status 'reviewing'.

7. **Review loop (CORE-05: isolated errors):** For each reviewable file, sequentially:
   a. Send `REVIEW_PROGRESS` with currentFile, fileIndex (1-based), totalFiles, status 'reviewing'.
   b. Try (wrapped in `retryWithBackoff` from `./retry` with `{ maxRetries: 2, baseDelayMs: 1000 }`):
      - Call `getFileContent(prInfo, change.item.path, prDetails.sourceCommitId)`.
      - Call `reviewSingleFile(change.item.path, content, changeTypeStr, apiKey)` from `./llm-reviewer`.
      - Send `REVIEW_PROGRESS` with status 'posting-comments'.
      - Call `postFileFindings(prInfo, change.item.path, review.findings, iterationId)` from `./comment-mapper`.
      - Push a `SingleFileResult` with status 'success', findings, and fileSummary.
   c. Catch: Push a `SingleFileResult` with status 'error' and the error message. Do NOT rethrow -- continue to next file.
   d. Send `REVIEW_FILE_COMPLETE` with the FileReviewResult (filePath, status, findingCount or error).

8. **Build and post summary:** Call `buildSummaryMarkdown(results, prDetails.title)` from `./comment-mapper`. Call `postSummaryComment(prInfo, summaryMarkdown)` from `@/lib/ado-api/threads`. Wrap in try/catch -- if summary posting fails, log but don't throw.

9. **Send completion:** Build `ReviewSummary` by tallying results. Send `REVIEW_COMPLETE` with the summary.

Import all types from the appropriate modules. The `onProgress` callback receives `PortMessage` objects -- the background handler forwards these directly over the port.

**Update `entrypoints/background.ts`:**

Keep ALL existing code (handler registry, CHECK_AUTH, SAVE_PAT handlers, console.log). Add a port connection listener INSIDE the `defineBackground` callback, AFTER the existing `browser.runtime.onMessage.addListener`:

```typescript
// Handle long-lived port connections for review sessions
browser.runtime.onConnect.addListener((port) => {
  if (port.name !== 'review') return;

  port.onMessage.addListener(async (msg: PortMessage) => {
    if (msg.type === 'START_REVIEW') {
      try {
        await runReview(msg.payload.prInfo, (progress) => {
          try {
            port.postMessage(progress);
          } catch {
            // Port may have disconnected -- ignore
          }
        });
      } catch (error) {
        try {
          port.postMessage({
            type: 'REVIEW_ERROR',
            payload: {
              message: error instanceof Error ? error.message : String(error),
            },
          });
        } catch {
          // Port disconnected
        }
      }
    }
  });

  port.onDisconnect.addListener(() => {
    console.log('[PEP Review] Review port disconnected');
  });
});
```

Import `runReview` from `@/lib/review/orchestrator` and `PortMessage` from `@/shared/messages` at the top of the file.

IMPORTANT: Every `port.postMessage` call must be wrapped in try/catch because the port may disconnect at any time (user navigates away mid-review). This prevents unhandled errors in the service worker.

IMPORTANT: Do NOT use `async` on the `browser.runtime.onConnect.addListener` callback itself -- only the inner `port.onMessage.addListener` callback is async.
  </action>
  <verify>
Run `pnpm exec tsc --noEmit` -- must pass. Verify `orchestrator.ts` imports from all ADO API modules and review modules. Verify `background.ts` has both `onMessage` listener (existing) and `onConnect` listener (new). Verify the word `retryWithBackoff` appears in `orchestrator.ts`.
  </verify>
  <done>
orchestrator.ts coordinates the full review pipeline: fetches PR data, filters non-code files, reviews each file with retry, posts inline comments, builds and posts summary. Error isolation per file (CORE-05). background.ts handles port connections and forwards progress messages. The pipeline is end-to-end functional from a port message trigger. All type-checks pass.
  </done>
</task>

</tasks>

<verification>
After both tasks complete:

1. `pnpm exec tsc --noEmit` passes with zero errors
2. `lib/review/` now has 8 files total (5 from Plan 01 + llm-reviewer.ts, comment-mapper.ts, orchestrator.ts)
3. `entrypoints/background.ts` has both onMessage (one-shot) and onConnect (port) listeners
4. `orchestrator.ts` calls: getOpenAiApiKey, getPrDetails, getLatestIterationId, getChangedFiles, shouldSkipFile, shouldSkipByChangeType, getFileContent, reviewSingleFile, postFileFindings, buildSummaryMarkdown, postSummaryComment, retryWithBackoff
5. `comment-mapper.ts` validates line numbers before posting (finding.line > 0)
6. `llm-reviewer.ts` uses `generateText` with `Output.object({ schema: FileReviewSchema })`
7. All `port.postMessage` calls in background.ts are wrapped in try/catch
</verification>

<success_criteria>
The complete review pipeline runs from a START_REVIEW port message to REVIEW_COMPLETE. Each changed file is fetched, sent to OpenAI, findings posted as inline comments, and progress reported. Non-code files are skipped. Failed files don't stop the pipeline. A summary comment is posted at the end. The background service worker stays alive via the port connection.
</success_criteria>

<output>
After completion, create `.planning/phases/02-pr-review-pipeline/02-02-SUMMARY.md`
</output>
